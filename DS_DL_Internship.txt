Day 1:
Setup Day

Day 2: Data Strategies
- Python Venv Setup and Implementation
- Jupyter notebook setup in Day Folder
- Dataset Importing
- Data Imputation
- Label Encoding
- One Hot Encoding

Day 3: Machine Learning
- Types of Analysis: Descriptive and Predictive
- Understanding Predictive Analysis with Examples
- Understanding Supervised and Unsupervised Learning (Solving Seen/Known[Supervised] and Unknown[Unsupervised] Problems)
- A/B Testing: Train and Test (Comparison in Supervised and Unsupervised)
--	90/10 Data Rule in  Train and Test Data (Supervised) [Ability to derive an answer based on trained data question]
---		Contains answer for all questions when trained
--	60/40 Data Rule in Train and Test  Data (Unsupervised) [Deriving answer from a certain pattern]
---		Regression:
----			Ability to predict continuous values
---		Classification:
----			Ability to predict discrete values
- Understanding Regression: Parametric and Non-parametric
--	Parametric: Linear, Linear discriminate Analysis (Traceable Method of Prediction)
--	Non-Parametric: RandomFirst, k-neighbor, SVR, QDA (Non-Traceable Method of Prediction)
- Understanding Parametric Regression: Linear Regression
--	Tracing a Common Line that could represent all points in a data, thereby plotting a linear function
--	y = a + bx + e (Known: x, y)(Unknown: a, b)(e: Error)
--	Implementation using Python
- Understanding Multi-Linear Regression [Least Squares Method] (Find Video Examples)
--	y^ = ÃŸ^
-- 	((X^T . X )^-1 . X^T) . y [Where X is a matrix of all Columns]

Day 4: PostgreSQL
- Understanding SQL
- Difference btw Excel and Database
- Structure of SQL and Relational Database Management System [RDBMS]
- Data Relations using Data Diagram
- Understanding Data Types
--	char, varchar, int, float, bool, date, datetime
- Understanding Primary and Foreign Keys and their Differences
- Understanding Constraints
- Operating on PostgreSQL
- Operators in SQL
- Refer PPT

Day 5: PostgreSQL continuation



Day 6: Understanding Model Validation
- Checking Validity of a Model
- Eg: Loss Function and Loss Metric in Regression (Mean Squared Error)
- Mean Squared Error (MSE)
--  MSE = 1/N(Sum(i=1)(n){yi - ^yi})
--  Error = yi - ^yi
--  Error^2 = (yi - ^yi)^2
- Converging Loss Function (Loss Optimisation)
--  Gradient Descent (Loss Optimiser)
---     Loss = 1/N(Sum(i=1)(n){yi - {b0 + b1(xi)}})
----        Step1: Perfrom Convergence
----        Step2: Take Learning Value Arbitrary 
----        Step3: Multiply derivative with alpha(a) and subtract from original parameters (b1 & b0)
----        Epoch: Iterations of For loop
---     Implementation
- Understanding Classification: Using machine learning to Classify in Problems
--  Logistic Regression:  Binary Classification / Supervised / Parametric
--  Converting Linear Line Eq into non-linear function (Sigmoid Function)

Day 7:
- Understanding Deep Learning
--  Artificial Neural Network (ANN)
---     Nodes and Coonnections
---     Weights, Biases
- Understanding Feed Forward ANN and Back Propogation
- Classification in ANN
--  Examples of MNIST - Dataset of Hadwritten digits
---     Activation Function -  Convert linear to Non-Linear Functions
----        ReLU : x < 0 = 0; x > 0 = x
----        tanh : f(x) = (e^x - e^-x)/(e^x + e^-x)
----        sigmoid : o(x) = 1/(1 + e^-x)
----        SoftMax : SM(x) =  e^x/sum(e^x) => ArgMax(SM(x))
-----           It sustains multiple label values in 0-1 probablity Range
-----           pick highest probablity value using max arg function eg: ArgMax